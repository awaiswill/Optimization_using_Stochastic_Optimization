## Optimization_using_Momentum based Optimizers Adam, AdamW and AdamW+
This work is associated with research on stochastic optimization of Deep Learning models. Stochastic optimization is achieved through Regularization methods.
We will consider three main optimizers namely SGD with momentum, Adam and AdamW. 

https://www.kaggle.com/awaisjaved/dga-detection-optimizers/edit


